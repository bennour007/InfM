---
title: "Untitled"
output: pdf_document
date: "2024-05-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse)
```

## Abstract

## Introduction (containing literature selection and field introduction)

Text mining is a powerful tool that can be used for various objectives. The power of such tool has been seen in 2020 when researchers all over the world needed information and details of studies in many medicine-related fields for their endeavor in developing a vaccine for covid-19. Additionally, the emergence of large language models (or LLMs) is another realisation taken to the extreme of text mining and language modeling. In academia, researchers can also leverage such tools to inform their decisions in pursuing a research field for instance. Eventually everything, including text, boils down to data and with the proper tools we can analyse it and find insightful information. In this context, this paper aims at doing just that. In fact, I will be using text mining techniques to delve into the Economic complexity literature as it relates directly to the topic of my thesis. The idea here is to cluster a set of publications' abstracts with a topic modeling technique (namely the Latent Dirichlet Allocation (LDA) algorithm). The idea is to explore this body of literature beyond the keywords and the subject categories (although useful to consider), by exploring analytically the relevance and significance of the words and terms used in a set of a 118 abstract extracted from the Web of Science (WoS) database, and combined into a single text corpus(more details in the following sections).\
The justification for such an approah is three folds. First, the economic complexity literature gained a tremendous momentum since its inception in 2007. Second, the problems treated in this body of literature are very diverse and the contributions are various and marginal in many cases, thus by having more context from the paper I can identify hidden areas of research that are not explicitly treated. And third, it's very interesting to see the trends around the topics, and what are the current main concerns of these studies. Another aspect of the importance of such an approach is the fact the Economic Complexity paradigm offers a set of network analysis tools that can be leveraged in any sense and direction. For instance there are studies that unravel the relation of complex networks and emissions and others with gdp growth. Thus to investigate the trends and the directions of this literature body, in bulk and without the biasis that might be find in reviews of literature, text mining provides the best alternative. The remaining of the paper is as follows, in section 2 I present the data in more details and the preprocessing that took place to become suitable for the analysis which I will uncover and detail in section 3. In section 4 I will present the main results and interpret them. In section 4 I will provide visualisations for the results and discuss them, and finally I conclude the paper in section 5.

## Data preprocessing (make suitable for text mining)

The data used in this paper was extracted from the WoS (Web of Science) databse, the extraction process is quite simple and is done directly from the web interface of the website after a search query that targets papers investigating complexity and relatedness using the following :

```         
TS=("economic complexity" OR "relatedness")  AND AK =("economic complexity" OR "relatedness")
```

The result of this search yielded 1057 papers in total after limiting the years to the range between 2007 and 2024 and the WoS subject category to subjects in economics, regional and urban planning as well as geography of innovation, I then export the metadata of these papers (Author, Title and Abstract) into multiple tab delimited files with `.txt` extension. The aim of this work is to target the complexity literature through the texts of the abstracts of the extracted data. Thus I merge all the abstract into one corpus and start the preprocessing from that point.

The preprocessing starts with the few typical tasks, I first lower all cases in the corpus text, I removed all the non english letters and extra spaces, as well as english stopwords as well as other words that might be over used the likes of (use, also, one, two, etc.). Additionally I proceed with the removing numbers and punctuation.

The second step of the preprocessing envolved the tokenisation of the corpus, this process usually is followed by the stemming procedure, but judging from the small data size and from the experiments I conducted in the process, I decided to avoid that. With these steps, I finalised the preprocessing phase with the creation of the term matrix.

![Word Cloud of the corpus](images/Screenshot from 2024-05-16 18-33-02.png)

## Generating topic models (tuning parameters and testing results)

For the topic modelling phase, I used the Latent Dirichlet Allocation (LDA) algorithm and tested it with both Gibbs sampling method and VEM (Variational Expectation-Maximization) method. The Gibbs sampling method is a Markov Chain Monte Carlo procedure that is used with LDA to estimate the posterior distribution of the hidden values, the topics in our case. This method is quite efficient for small datasets such as the one used here wheras the VEM method uses variational inference to estimate the posterior distribution as tt transforms the inference problem into an optimization problem, The VEM method is more reliable when dealing with huge corpus of texts, however it provides an approximation to the posterior probabilities making it faster than the Gibbs method but at the same time less accurate. Thus for the purposes of this paper I will use the Gibbs method of sampling as it aligns with the data I have. However, I should note that this method is more effective when is conducted to model multiple documents rather than one corpus such as the case in this paper.

For this phase, I created a grid of parameters that controls the model"s accuracy to model the topics in the corpus:

-   Alpha ($\alpha$): is a hyperparameter that influences the distribution of topics within documents. It is a parameter of the Dirichlet prior on the per-document topic distributions.

-   Iter (Iterations): The number of iterations (iter) is a parameter that specifies how many times the Gibbs sampling process should be repeated.

-   K (topics): The number of topics to model for each model variation

To optimize the Latent Dirichlet Allocation (LDA) model for topic modeling, a comprehensive set of parameter configurations was tested using Gibbs sampling. The parameters included different values for alpha, the Dirichlet prior for document-topic distribution, ranging from 0.01 to 5, specifically: 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, and 5. Additionally, the number of iterations for the Gibbs sampler was varied extensively, testing values of 100, 200, 300, 400, 500, 1000, and 2000 iterations to ensure convergence and stability of the results. The number of topics k was also varied to explore different insights of topic decomposition, with values of 3, 4, 5, 6, and 7 topics being tested. This gets us eventually 280 combinations alltogether of these parameters which is quite the process to evaluate manually, although eventually I used the Arun2010 metric to determine the best number of topics I can have given the term matrix, and 4 topics ended up being the most optimal number of topics for the corpus. Additionally, and to ease up the evaluation of the models, I rerun the models again with 4 topics and the same grid of values for the parameters alpha and number of iterations, and calculated the perplexity measure for these models (56 combinations) to look at the most coherent ones at least mathematically before evaluating the content of the topics themselves. The top 10 models with the lowest perplexity measure are shown in table 1.

```{r echo=FALSE, message=FALSE, warning=FALSE}
tab1 <- read_csv(here::here("results", "top_perplexity.csv"))
tab1 %>% 
  gt::gt(rowname_col = "Model") %>% 
  gt::tab_header(
    title = "Table 1: Models' perpelexity (best 10 models)"
  )
```

Additionally, Table 2 provides a glimpse on the terms in each topic for the top 3 models with the least perplexity values. Each model provides 4 topics with the top 7 terms each.

```{r echo=FALSE, message=FALSE, warning=FALSE}
tab2 <- read_csv(here::here("results", "top3_topics.csv"))
tab2 %>% 
  gt::gt(groupname_col = "Model") %>% 
  gt::tab_header(
    title = "Table 2: Best 3 models (topics and terms)"
  )
```

## Topic identification (labeling and matching to scientific concepts)

The modeled topics in table 2 provide directional insights on the literature on Economic Complexity. From the results obtained in table one (and detailed in figure 1), I decided to adopt the topics from the first model (alpha_2_iter_1000). As indicated by the model name, the parameters are $\alpha = 2$ and the number of iterations is 1000. The reason for this choice is, apart from having the lowest perplexity of all the other models, it also provides the most diverse topics and meaningful terms that actually might align with the literature. For instance the first topic (with the terms: gdp, part, entry, representation, mechanism, density, cluster) can be interpreted as the body of this literature that targets the relatedness density of clusters of agents (institutions, regions, countries, etc.) how this density is affected by the entery to a new area of production or knowldge, and essentially explaining the gdp changes by those dynamics. This topic can be called *relatedness* which is one major component of the Economic Complexity paradigm and its literature. The second topic (containing terms : individual, proposes, performed, year, revealed, contexts, patents) is describing the contexts of the specialisation patterns (revealed as in Revealed Comparative Advantage) and the dynamics of these specialisation (over time: year) by the means of patents' applications data. This topic describes the mechanics of specialisations of innovative firms and thus can be named the *innovation* topic, which is one of the most treated areas in this paradigm. Moreover, the third topic (containing: n, practices, focusing, distribution, probability, way, reveals) seems to target the statistical and mathematical sides of the toolkit provided by the paradigm. Indeed, many models have been developed in this body of literature to assess the probability of entery to a new field of knowledge given specific variables, such as the level of relatedness density, complexity, and in other instances even gdp. In fact some leading scholars in this body of literature argued continuously about the need to develop new methodologies that allow researchers to properly investigate the contribution of different factors in increasing the likelihood of entery to a new field in a more dynamic sense. Being able to determine the optimal diversification and specialisation strategy (over time and space) given the corpus of the knowledge that a study captures is a powerful concept and has been lately addressed in many papers. Thus this third topic will be named the *technical* cluster. Finally, the forth topic (with the terms: economic, relatedness, complexity, study, results, countries, research) represent the classic cluster of this literature focusing mainly on the explaining economic performance (in terms of GDP, or attractiveness for FDI and trade or in many instances emission and energy consmption) with the characteristics of a country in terms of its complexity (the level of diverse specialisations) and relatedness density (the diversity of related knowledge in which a country has a comparative advantage). Thus this topic should be called the *complexity* topic.

## Visual exploration (topic patterns by time, topic relations)

In this section I will try to interpret with a bit of nuance the wights of each term in each topic of the previously modeled corpus. In fact a summary of these weights can be found in Figure 2. We can see from the figure that topic 4 (*Complexity*) contains the terms with the highest weights across all the 4 topics. This is understandable given that Economic Complexity as a literature body is fairly new and that more contributions in the core of the literature is being done. Although the term "economic" seems to have the greatest weight of all, which also makes perfect sense since first, the term is part of the naming of the paradigm itself, and second this paradigm mainly provides tools that target the analysis of economic activity in general. In the same context, for topic 1 (the relatedness topic) the terms gdp, part, and entery are the highest defining terms of this cluster meaning that the economic performance and entery to new areas of knowledge are of major concern in this topic, although I cannot relate the relevance of the term "part" in this topic to anything in the literature. Similarly with topic 2 ("Innovation") the highest contributing term to this cluster is "individual", a term that is not necessairly meaningful in contrast with the other terms. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot1 <- read_rds(here::here("results", "plot_weights.rds"))

plot1+
  labs(title = "Figure 2: Prior weights of each words in each topic") 
  # guides(
  #   fill = guide_legend(
  #     title = "Topic", 
  #     override.aes = list(fill = c("Relatedness", "Innovation", "Technical", "Complexity"))
  #   )
  # )

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot2 <- read_rds(here::here("results", "wordsd.rds"))

plot2 +
  labs(title = "Figure 3: Treemap of the corpus")
```

## Conclusions

In conclusion, this work is a mere trial to model topics of a sample of abstracts of the body of literature in Economic Complexity. In this paper, I used preprocessing techniques to create a single corpus that incorporates all the abstracts, then I used LDA with different combinations of parameters, from which I chose the model with the parameters yielding the lowest perplexity measure. The results that this modeling gave are mixed, on one hand the forth topic ("Complexity") provides a general and meaningful outlook with the classic terms that can be found in the papers of this body of literature. However for the other topics, and despite my effort to initially bend the terms and interpret their meaning in the context of the literature itself, the weights associated with each of these terms show unconsistencies and fuzziness that cannot be properly interpreted. This can be due to various factors, One usually in the abstracts the authors try to justify the study and present the methodology and results in a brief manner, and thus the abstracts might not properly capture the terms that are used across the full papers. Second, the preprocessing can be improved especially regarding the stemming and the stop words. Third, other options of modeling algorithms should be considered in order to improve the results and their interpretability. Finally, and probably most importantly, more data is needed, the size of 1000 publication can be decent if it contains the entirety of the publication and not just the abstracts. Eventually, this excercice was an interesting application to topic modeling on a body of literature I'm working with for my thesis. Initially the idea was to try and compare WoS subject categories with the topics, although the initial data contains 35000 records of published studies from 152 category, my intuition was that it would be too much to model such corpus. After this paper, the next steps would be to actually test the same approach but on different corpora that represent unique subject categories. 
