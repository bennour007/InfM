{
    "type": [
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        0,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        3,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        2,
        2,
        0,
        1,
        2,
        2,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        3,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        1,
        0,
        1,
        3,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        0,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        1,
        2
    ],
    "data": [
        "  guides(fill=\"none\") +",
        "+ ",
        "  geom_treemap() +",
        "+ ",
        "  geom_treemap_text(colour = \"white\",",
        "+ ",
        "                    place = \"centre\",",
        "+ ",
        "                    size = 15) +",
        "+ ",
        "  scale_fill_viridis_d()",
        "\u001B[38;5;232mSelecting by count\u001B[39m\n",
        "> ",
        "# SAVE THIS AND USE IT LATER LOOKS GREAT",
        "> ",
        "wordcloud2::wordcloud2(words)",
        "> ",
        "# Convert DTM to matrix",
        "> ",
        "dtm_matrix <- as.matrix(dtm)",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 10",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 10)",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 10 Ã— 10\u001B[39m\n   `Topic 1`    `Topic 2` `Topic 3` `Topic 4` `Topic 5` `Topic 6` `Topic 7` `Topic 8` `Topic 9` `Topic 10`\n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m",
        " 1\u001B[39m facilitate   brazil    intervenâ€¦ economic  rate      directly  benchmark helps     becomes   highest   \n\u001B[38;5;250m 2\u001B[39m computed     enables   entrepreâ€¦ relatednâ€¦ swedish   confirm   entries   carried   lexical   far       \n\u001B[38;5;250m 3\u001B[39m focal        latent    places    complexiâ€¦ studies   dimension computatâ€¦ infrastrâ€¦ methodolâ€¦ extraction\n\u001B[38;5;250m 4\u001B[39m voice        pressure  meaningfâ€¦ study     members   conduct   relate    competitâ€¦ threshold weighting \n\u001B[38;5;250m 5",
        "\u001B[39m better       principle finance   results   occupatiâ€¦ hidden    rise      final     italy     drive     \n\u001B[38;5;250m 6\u001B[39m sensitive    intermedâ€¦ progress  countries adaptive  analyzes  designs   accuracy  perceptiâ€¦ enter     \n\u001B[38;5;250m 7\u001B[39m shown        involvemâ€¦ classes   data      linguistâ€¦ lexical   regressiâ€¦ construcâ€¦ rdf       footprint \n\u001B[38;5;250m 8\u001B[39m transportatâ€¦ makes     expressed research  awareness loss      argues    instrumeâ€¦ away      events    \n\u001B[38;5;250m 9\u001B[",
        "39m chains       unsupervâ€¦ healthcaâ€¦ paper     descriptâ€¦ problems  en        integratâ€¦ childrens strengths \n\u001B[38;5;250m10\u001B[39m knowledgeinâ€¦ addressed adaptatiâ€¦ knowledge preferenâ€¦ assesses  inherent  merging   complexiâ€¦ absence   \n",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 5",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 10)",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 10 Ã— 5\u001B[39m\n   `Topic 1`   `Topic 2`    `Topic 3`    `Topic 4`   `Topic 5`    \n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \n\u001B[38;5;250m 1\u001B[39m economic    evolution    increasingly objective   short        \n\u001B[38;5;250m 2\u001B[39m relatedness processing   examining    original    ways         \n\u001B[38;5;250m 3\u001B[39m complexity  considered   demand ",
        "      qualitative educational  \n\u001B[38;5;250m 4\u001B[39m study       others       leads        able        result       \n\u001B[38;5;250m 5\u001B[39m results     choice       sport        fully       paths        \n\u001B[38;5;250m 6\u001B[39m countries   workers      respectively take        sophisticated\n\u001B[38;5;250m 7\u001B[39m data        correlated   distribution fuel        documents    \n\u001B[38;5;250m 8\u001B[39m research    synergies    gains        leading     larger       \n\u001B[38;5;250m 9\u001B[39m paper       need         growing      terms   ",
        "    estimation   \n\u001B[38;5;250m10\u001B[39m knowledge   associations interactions underlying  th           \n",
        "> ",
        "# Get the topic distribution for each document",
        "> ",
        "doc_topics <- as.data.frame(lda_model_Gibbs@beta) %>% as_tibble()",
        "> ",
        "tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\")",
        "> ",
        "?LDA",
        "> ",
        "tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\")",
        "Error: object 'lda_model_Gibbs' not found\n",
        "> ",
        "pacman::p_load(tidyverse, mallet, tidytext, tm, treemapify, topicmodels)",
        "> ",
        "> ",
        "> ",
        "data1 <- read_tsv(here::here(\"data\", \"savedrecs1000.txt\")) %>%  select(TI, AB)",
        "\r\u001B[1mindexing\u001B[0m \u001B[34msavedrecs1000.txt\u001B[0m [===============================] \u001B[32m122.88GB/s\u001B[0m, eta: \u001B[36m 0s\u001B[0m\r                                                                                                                    \r\u001B[1mRows: \u001B[22m\u001B[34m1000\u001B[39m \u001B[1mColumns: \u001B[22m\u001B[34m71\u001B[39m\n\u001B[36mâ”€â”€\u001B[39m \u001B[1mColumn specification\u001B[22m \u001B[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
        "€â”€â”€\u001B[39m\n\u001B[1mDelimiter:\u001B[22m \"\\t\"\n\u001B[31mchr\u001B[39m (31): PT, AU, BE, GP, AF, CA, TI, SO, SE, CT, CY, CL, SP, HO, AB, RI, OI,...\n\u001B[32mdbl\u001B[39m  (5): PY, VL, SU, BP, PM\n\u001B[33mlgl\u001B[39m (35): BA, BF, BS, LA, DT, DE, ID, C1, C3, RP, EM, FU, FP, FX, CR, NR, TC,...\n\n\u001B[36mâ„¹\u001B[39m Use `spec()` to retrieve the full column specification for this data.\n\u001B[36mâ„¹\u001B[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
        "> ",
        "data2 <- read_tsv(here::here(\"data\", \"savedrecs1057.txt\")) %>%  select(TI, AB)",
        "\r\u001B[1mindexing\u001B[0m \u001B[34msavedrecs1057.txt\u001B[0m [================================] \u001B[32m12.06GB/s\u001B[0m, eta: \u001B[36m 0s\u001B[0m\r                                                                                                                    \r\u001B[1mRows: \u001B[22m\u001B[34m57\u001B[39m \u001B[1mColumns: \u001B[22m\u001B[34m71\u001B[39m\n\u001B[36mâ”€â”€\u001B[39m \u001B[1mColumn specification\u001B[22m \u001B[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â",
        "”€â”€\u001B[39m\n\u001B[1mDelimiter:\u001B[22m \"\\t\"\n\u001B[31mchr\u001B[39m (29): PT, AU, BE, GP, AF, TI, SO, SE, CT, CY, CL, SP, HO, AB, RI, OI, SN,...\n\u001B[32mdbl\u001B[39m  (5): PY, VL, BP, AR, PM\n\u001B[33mlgl\u001B[39m (37): BA, BF, CA, BS, LA, DT, DE, ID, C1, C3, RP, EM, FU, FP, FX, CR, NR,...\n\n\u001B[36mâ„¹\u001B[39m Use `spec()` to retrieve the full column specification for this data.\n\u001B[36mâ„¹\u001B[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
        "> ",
        "> ",
        "main_data <- bind_rows(",
        "+ ",
        "  data1, data2",
        "+ ",
        ") %>% ",
        "+ ",
        "  filter(!is.na(AB)) %>%   #  REMOVE EMPTY ABSTRACTS FORM THE DATA %>% ",
        "+ ",
        "  distinct(TI, .keep_all = T)",
        "> ",
        "> ",
        "# EXTRACTING ALL ABSTRACTS INTO ONE SINGLE CORPUS",
        "> ",
        "corpus_raw <- main_data %>% ",
        "+ ",
        "  select(AB) %>% ",
        "+ ",
        "  mutate(AB = as.character(AB)) %>% ",
        "+ ",
        "  str_c(collapse = \" \")",
        "Warning message:\nIn stri_c(list(AB = c(\"In recent years economic complexity has grown into an active field of fundamental and applied research. Yet, despite important advances, the policy implications of economic complexity can remain unclear or misunderstood. Here I organize the policy implications of economic complexity in a framework grounded on 4 Ws: what approaches, focused on identifying target activities and/or locations; when approaches, focused on the timing of related and unrelated diversification",
        "; where approaches, focused on the geographic diffusion of knowledge; and who approaches, focused on the role played by agents of structural change. The goal of this paper is to provide a framework that groups, organizes, and clarifies the policy implications of economic complexity and facilitates its continued use in regional and international development.\",  :\n  argument is not an atomic vector; coercing\n",
        "> ",
        "  ",
        "> ",
        "> ",
        "# Step 1: Create a corpus",
        "> ",
        "corpus <- Corpus(VectorSource(corpus_raw))",
        "> ",
        "> ",
        "# Step 2: Text transformation",
        "> ",
        "> ",
        "# convert to lower case",
        "> ",
        "mydata <- tm_map(corpus, content_transformer(tolower))",
        "Warning message:\nIn tm_map.SimpleCorpus(corpus, content_transformer(tolower)) :\n  transformation drops documents\n",
        "> ",
        "# remove anything other than English letters or space",
        "> ",
        "removeNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]\", \"\", x)",
        "> ",
        "mydata <- tm_map(mydata, content_transformer(removeNumPunct))",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, content_transformer(removeNumPunct)) :\n  transformation drops documents\n",
        "> ",
        "# remove stopwords",
        "> ",
        "mydata <- tm_map(mydata, removeWords, stopwords(\"english\"))",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeWords, stopwords(\"english\")) :\n  transformation drops documents\n",
        "> ",
        "#u can create custom stop words using the code below.",
        "> ",
        "myStopwords <- c(setdiff(stopwords('english'), c(\"r\", \"big\")),",
        "+ ",
        "                 \"eeg\", \"les\", \"c\", \"ie\", \"one\", \"e\", \"s\", \"co\", \"would\", \"will\", \"can\", ",
        "+ ",
        "                 \"however\", \"moreover\", \"use\", \"see\", \"used\", \"using\", \"via\", \"amp\", \"ss\", \"algal\",",
        "+ ",
        "                 \"use\", \"two\", \"also\", \"based\", \"key\", \"find\")",
        "> ",
        "mydata <- tm_map(mydata, removeWords, myStopwords)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeWords, myStopwords) :\n  transformation drops documents\n",
        "> ",
        "# remove extra whitespace",
        "> ",
        "mydata <- tm_map(mydata, stripWhitespace)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, stripWhitespace) :\n  transformation drops documents\n",
        "> ",
        "# Remove numbers",
        "> ",
        "mydata <- tm_map(mydata, removeNumbers)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeNumbers) :\n  transformation drops documents\n",
        "> ",
        "# Remove punctuations",
        "> ",
        "mydata <- tm_map(mydata, removePunctuation)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removePunctuation) :\n  transformation drops documents\n",
        "> ",
        "> ",
        "# inspect(mydata)",
        "> ",
        "> ",
        "# Step 3: Tokenization (optional)",
        "> ",
        "> ",
        "corpus_text <- sapply(mydata, as.character)",
        "> ",
        "tidy_data <- tibble(text = corpus_text)",
        "> ",
        "tidy_data <- tidy_data %>%",
        "+ ",
        "  unnest_tokens(word, text)",
        "> ",
        "> ",
        "# this was not a good idea",
        "> ",
        "# corpus2 <- tm_map(mydata, Boost_tokenizer)",
        "> ",
        "# inspect(corpus2)",
        "> ",
        "# Step 4: Stemming or Lemmatization (optional)",
        "> ",
        "# I FOUND THIS STEP TO CREATE SOME PROBLEMS WITH THE WORDS",
        "> ",
        "# corpus2 <- tm_map(corpus2, stemDocument)  # Stemming",
        "> ",
        "> ",
        "# Step 5: Create Document-Term Matrix (DTM)",
        "> ",
        "dtm <- DocumentTermMatrix(tidy_data)",
        "> ",
        "> ",
        "# inspect(dtm)",
        "> ",
        "> ",
        "# WORD FREQUENCY IN THE CORPUS",
        "> ",
        "words <- tidy_data %>% ",
        "+ ",
        "  group_by(word) %>% ",
        "+ ",
        "  summarise(count= n()) %>% ",
        "+ ",
        "  arrange(desc(count))",
        "> ",
        "  ",
        "> ",
        "> ",
        "# VISUALISATION OF THE FREQUENCY",
        "> ",
        "> ",
        "# MAKE THIS LOOK BETTER",
        "> ",
        "words %>% ",
        "+ ",
        "  top_n(100) %>% ",
        "+ ",
        "  ggplot(aes(area = count, fill = word, label = word)) +",
        "+ ",
        "  guides(fill=\"none\") +",
        "+ ",
        "  geom_treemap() +",
        "+ ",
        "  geom_treemap_text(colour = \"white\",",
        "+ ",
        "                    place = \"centre\",",
        "+ ",
        "                    size = 15) +",
        "+ ",
        "  scale_fill_viridis_d()",
        "\u001B[38;5;232mSelecting by count\u001B[39m\n",
        "> ",
        "> ",
        "# SAVE THIS AND USE IT LATER LOOKS GREAT",
        "> ",
        "wordcloud2::wordcloud2(words)",
        "> ",
        "> ",
        "> ",
        "## LET'S GO AFTER THE TOPIC MODELLING NOW!",
        "> ",
        "> ",
        "> ",
        "# Convert DTM to matrix",
        "> ",
        "dtm_matrix <- as.matrix(dtm)",
        "> ",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 5",
        "> ",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 10)",
        "> ",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 10 Ã— 5\u001B[39m\n   `Topic 1`  `Topic 2`   `Topic 3`   `Topic 4`     `Topic 5`    \n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \n\u001B[38;5;250m 1\u001B[39m conducted  economic    explaining  without       density      \n\u001B[38;5;250m 2\u001B[39m lower      relatedness function    comprehensive heterogeneous\n\u001B[38;5;250m 3\u001B[39m mechanisms complexity  digital     d",
        "ataset       larger       \n\u001B[38;5;250m 4\u001B[39m interest   study       qualitative provide       curve        \n\u001B[38;5;250m 5\u001B[39m synergies  results     primary     suggest       extended     \n\u001B[38;5;250m 6\u001B[39m acquirers  countries   involved    light         relational   \n\u001B[38;5;250m 7\u001B[39m discuss    data        nonlinear   similarity    decreases    \n\u001B[38;5;250m 8\u001B[39m required   research    provide     last          efficient    \n\u001B[38;5;250m 9\u001B[39m selection  paper       news        subject       mediate",
        "d     \n\u001B[38;5;250m10\u001B[39m efforts    knowledge   turkish     estimation    decision     \n",
        "> ",
        "  ",
        "> ",
        "> ",
        "> ",
        "# Get the topic distribution for each document",
        "> ",
        "doc_topics <- as.data.frame(lda_model_Gibbs@beta) %>% as_tibble()",
        "> ",
        "> ",
        "tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\")",
        "> ",
        "tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\")",
        "> ",
        "as.data.frame(lda_model_Gibbs@beta) %>% as_tibble()",
        "\u001B[38;5;246m# A tibble: 5 Ã— 11,101\u001B[39m\n      V1     V2     V3     V4    V5\n   \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m\n\u001B[38;5;250m1\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m4\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m4\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m4\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m4\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m4\u001B[39m\n\u001B[38;5;250m2\u001B[39m -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m",
        ".\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m\n\u001B[38;5;250m3\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m   -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m81\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m   -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m81\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m\n\u001B[38;5;250m4\u001B[39m  -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m88\u001B[39m  -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m23\u001B[39m  -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m88\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m3\u001B[39m  -\u001B[31m11\u001B[39m\u001B[",
        "31m.\u001B[39m\u001B[31m3\u001B[39m\n\u001B[38;5;250m5\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m\n\u001B[38;5;246m# â„¹ 11,096 more variables: V6 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V7 <dbl>, V8 <dbl>, V9 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V10 <dbl>, V11 <dbl>, V12 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V13 <dbl>, V14 <dbl>, V15 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V16 <dbl>, V17 <dbl>, V18 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V19 <dbl>,",
        " V20 <dbl>, V21 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V22 <dbl>, V23 <dbl>, V24 <dbl>, â€¦\u001B[39m\n\u001B[38;5;246m# â„¹ Use `colnames()` to see all variable names\u001B[39m\n",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 10 Ã— 5\u001B[39m\n   `Topic 1`  `Topic 2`   `Topic 3`   `Topic 4`     `Topic 5`    \n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \n\u001B[38;5;250m 1\u001B[39m conducted  economic    explaining  without       density      \n\u001B[38;5;250m 2\u001B[39m lower      relatedness function    comprehensive heterogeneous\n\u001B[38;5;250m 3\u001B[39m mechanisms complexity  digital     d",
        "ataset       larger       \n\u001B[38;5;250m 4\u001B[39m interest   study       qualitative provide       curve        \n\u001B[38;5;250m 5\u001B[39m synergies  results     primary     suggest       extended     \n\u001B[38;5;250m 6\u001B[39m acquirers  countries   involved    light         relational   \n\u001B[38;5;250m 7\u001B[39m discuss    data        nonlinear   similarity    decreases    \n\u001B[38;5;250m 8\u001B[39m required   research    provide     last          efficient    \n\u001B[38;5;250m 9\u001B[39m selection  paper       news        subject       mediate",
        "d     \n\u001B[38;5;250m10\u001B[39m efforts    knowledge   turkish     estimation    decision     \n",
        "> ",
        "## LET'S GO AFTER THE TOPIC MODELLING NOW!",
        "> ",
        "> ",
        "> ",
        "# Convert DTM to matrix",
        "> ",
        "dtm_matrix <- as.matrix(dtm)",
        "> ",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 5",
        "> ",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 4)",
        "> ",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 4 Ã— 5\u001B[39m\n  `Topic 1` `Topic 2`      `Topic 3`   `Topic 4`       `Topic 5`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m           \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m run       firms          economic    digital         causality\n\u001B[38;5;250m2\u001B[39m networks  central        relatedness diversification whereas  \n\u001B[38;5;250m3\u001B[39m pe        disambiguation complexity  emplo",
        "yees       obtain   \n\u001B[38;5;250m4\u001B[39m resource  emergence      study       place           indirect \n",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 5",
        "> ",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 7)",
        "> ",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 7 Ã— 5\u001B[39m\n  `Topic 1`  `Topic 2`     `Topic 3` `Topic 4`   `Topic 5`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m genetic    creation      aim       economic    program  \n\u001B[38;5;250m2\u001B[39m political  measuring     limited   relatedness form     \n\u001B[38;5;250m3\u001B[39m range      heterogeneity employee  complexity  primary  \n\u001B[38;5;25",
        "0m4\u001B[39m attitudes  african       feature   study       society  \n\u001B[38;5;250m5\u001B[39m patents    sport         city      results     startups \n\u001B[38;5;250m6\u001B[39m gas        shown         performed countries   explicit \n\u001B[38;5;250m7\u001B[39m resilience plays         turn      research    third    \n",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 4",
        "> ",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 7)",
        "> ",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 7 Ã— 4\u001B[39m\n  `Topic 1`    `Topic 2`   `Topic 3`   `Topic 4`       \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m           \n\u001B[38;5;250m1\u001B[39m given        economic    environment create          \n\u001B[38;5;250m2\u001B[39m evaluate     relatedness providing   issues          \n\u001B[38;5;250m3\u001B[39m analyze      complexity  real        smart           \n\u001B[38;5;250m4\u001B[39m insights     study       questi",
        "on    interest        \n\u001B[38;5;250m5\u001B[39m goals        results     benefits    support         \n\u001B[38;5;250m6\u001B[39m university   countries   age         frequency       \n\u001B[38;5;250m7\u001B[39m interactions data        decisions   originalityvalue\n",
        "> ",
        "run_LDA <- function(dtm_matrix, k, L) {",
        "+ ",
        "  # Run LDA using Gibbs sampling method",
        "+ ",
        "  lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "+ ",
        "  ",
        "+ ",
        "  # Get the top words for each topic",
        "+ ",
        "  top_words <- terms(lda_model_Gibbs, L)",
        "+ ",
        "  ",
        "+ ",
        "  # Convert top_words to tibble and remove unnecessary characters",
        "+ ",
        "  top_words_tibble <- top_words %>% ",
        "+ ",
        "    as_tibble() %>% ",
        "+ ",
        "    mutate(",
        "+ ",
        "      across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))",
        "+ ",
        "    )",
        "+ ",
        "  ",
        "+ ",
        "  # Return the table with topics and their top words",
        "+ ",
        "  return(top_words_tibble)",
        "+ ",
        "}",
        "> ",
        "run_LDA(dtm_matrix = dtm_matrix, K = 10, L = 10)",
        "Error in run_LDA(dtm_matrix = dtm_matrix, K = 10, L = 10) : \n  unused argument (K = 10)\n",
        "> ",
        "run_LDA(dtm_matrix = dtm_matrix, k = 10, L = 10)",
        "\u001B[38;5;246m# A tibble: 10 Ã— 10\u001B[39m\n   `Topic 1` `Topic 2`      `Topic 3`     `Topic 4` `Topic 5` `Topic 6` `Topic 7`\n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m 1\u001B[39m fact      predictive     correlations  achieve   economic  light     mediated \n\u001B[38;5;250m 2\u001B[39m weak      preferred",
        "      require       extracted relatednâ€¦ automatiâ€¦ places   \n\u001B[38;5;250m 3\u001B[39m child     drive          series        educators complexiâ€¦ relations discover \n\u001B[38;5;250m 4\u001B[39m gained    estimator      gdp           position  study     argued    map      \n\u001B[38;5;250m 5\u001B[39m suitable  infrastructure visualization alliances results   consisteâ€¦ middleinâ€¦\n\u001B[38;5;250m 6\u001B[39m mix       introduction   perform       basic     countries spread    history  \n\u001B[38;5;250m 7\u001B[39m single    databases      sd    ",
        "        exploitiâ€¦ data      supports  partners \n\u001B[38;5;250m 8\u001B[39m varying   firmlevel      reach         latent    research  leading   sheds    \n\u001B[38;5;250m 9\u001B[39m away      hidden         enter         outcome   paper     variation ushaped  \n\u001B[38;5;250m10\u001B[39m direct    spatial        substantially trajectoâ€¦ knowledge assesses  visual   \n\u001B[38;5;246m# â„¹ 3 more variables: `Topic 8` <chr>, `Topic 9` <chr>, `Topic 10` <chr>\u001B[39m\n",
        "> ",
        "run_LDA(dtm_matrix = dtm_matrix, k = 10, L = 10) %>%  write_rds(here::here(\"results\", \"m_10x10.rds\"))",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`      `Topic 2`    `Topic 3` `Topic 4`  \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \n\u001B[38;5;250m1\u001B[39m part           geographical make      economic   \n\u001B[38;5;250m2\u001B[39m transformation teaching     practices relatedness\n\u001B[38;5;250m3\u001B[39m construct      university   engage    complexity \n\u001B[38;5;250m4\u001B[39m corporate      eg           fdi       study      \n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 5)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 5 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2` `Topic 3` `Topic 4`    \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \n\u001B[38;5;250m1\u001B[39m economic    small     basic     empirically  \n\u001B[38;5;250m2\u001B[39m relatedness exports   markets   estimation   \n\u001B[38;5;250m3\u001B[39m complexity  promoting sets      comprehensive\n\u001B[38;5;250m4\u001B[39m study       china     real      short        \n\u001B[38;5;250m5\u001B[39m results",
        "     little    flows     frustration  \n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`    `Topic 2`     `Topic 3`   `Topic 4` \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m1\u001B[39m services     environmental economic    provides  \n\u001B[38;5;250m2\u001B[39m traditional  due           relatedness causality \n\u001B[38;5;250m3\u001B[39m investigated four          complexity  goal      \n\u001B[38;5;250m4\u001B[39m increased    association   study       estimation\n",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`    `Topic 2`     `Topic 3`   `Topic 4` \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m1\u001B[39m services     environmental economic    provides  \n\u001B[38;5;250m2\u001B[39m traditional  due           relatedness causality \n\u001B[38;5;250m3\u001B[39m investigated four          complexity  goal      \n\u001B[38;5;250m4\u001B[39m increased    association   study       estimation\n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`     `Topic 3` `Topic 4`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m economic    effectiveness climate   panel    \n\u001B[38;5;250m2\u001B[39m relatedness ontologies    users     proximity\n\u001B[38;5;250m3\u001B[39m complexity  implications  capture   goods    \n\u001B[38;5;250m4\u001B[39m study       recently      services  state    \n",
        "> ",
        "set.seed(444)",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`      `Topic 3`   `Topic 4`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m economic    particular     relation    dataset  \n\u001B[38;5;250m2\u001B[39m relatedness specialization children    de       \n\u001B[38;5;250m3\u001B[39m complexity  focused        experiences effective\n\u001B[38;5;250m4\u001B[39m study       estimate       tested      users    \n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`   `Topic 3`  `Topic 4`   \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \n\u001B[38;5;250m1\u001B[39m economic    decisions   main       investigated\n\u001B[38;5;250m2\u001B[39m relatedness statistical openness   document    \n\u001B[38;5;250m3\u001B[39m complexity  wordnet     particular ways        \n\u001B[38;5;250m4\u001B[39m study       experienced query      corporate   \n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1` `Topic 2` `Topic 3` `Topic 4`  \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \n\u001B[38;5;250m1\u001B[39m accuracy  measured  lead      economic   \n\u001B[38;5;250m2\u001B[39m although  advantage document  relatedness\n\u001B[38;5;250m3\u001B[39m value     set       despite   complexity \n\u001B[38;5;250m4\u001B[39m workers   engage    reveals   study      \n",
        "> ",
        "set.seed(454)",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`  `Topic 2`   `Topic 3`  `Topic 4`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m states     economic    domestic   students \n\u001B[38;5;250m2\u001B[39m retrieval  relatedness innovative size     \n\u001B[38;5;250m3\u001B[39m university complexity  conducted  rather   \n\u001B[38;5;250m4\u001B[39m end        study       relatively suggests \n",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1` `Topic 2`   `Topic 3`     `Topic 4`     \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \n\u001B[38;5;250m1\u001B[39m text      economic    reserved      increased     \n\u001B[38;5;250m2\u001B[39m intensity relatedness interest      environments  \n\u001B[38;5;250m3\u001B[39m national  complexity  comprehensive representation\n\u001B[38;5;250m4\u001B[39m yet       study       forms         estimation",
        "    \n",
        "> ",
        "set.seed(450)",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2` `Topic 3` `Topic 4` \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m1\u001B[39m economic    affect    focused   change    \n\u001B[38;5;250m2\u001B[39m relatedness source    stronger  changes   \n\u001B[38;5;250m3\u001B[39m complexity  proposes  linked    common    \n\u001B[38;5;250m4\u001B[39m study       certain   intrinsic dimensions\n",
        "> ",
        "set.seed(312)",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1` `Topic 2` `Topic 3`   `Topic 4`  \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \n\u001B[38;5;250m1\u001B[39m employees accuracy  sources     economic   \n\u001B[38;5;250m2\u001B[39m goals     thus      educational relatedness\n\u001B[38;5;250m3\u001B[39m markets   urban     focusing    complexity \n\u001B[38;5;250m4\u001B[39m practices energy    forms       study      \n",
        "> ",
        "set.seed(312)",
        "> ",
        "> ",
        "m10 <- run_LDA(dtm_matrix = dtm_matrix, k = 10, L = 10) ",
        "> ",
        "m05 <- run_LDA(dtm_matrix = dtm_matrix, k = 5, L = 4)   ",
        "> ",
        "m04 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 7) ",
        "> ",
        "m42 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 4)   ",
        "> ",
        "> ",
        "> ",
        "m_10 %>% write_rds(here::here(\"results\", \"m_10x10.rds\"))",
        "Error: object 'm_10' not found\n",
        "> ",
        "m10",
        "\u001B[38;5;246m# A tibble: 10 Ã— 10\u001B[39m\n   `Topic 1` `Topic 2`      `Topic 3`    `Topic 4`  `Topic 5` `Topic 6` `Topic 7`\n   \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m 1\u001B[39m exchange  developing     conclusion   robustness original  beyond    existence\n\u001B[38;5;250m 2\u001B[39m series    interacti",
        "on    shown        distinct   inverted  employmeâ€¦ followed \n\u001B[38;5;250m 3\u001B[39m r         methodological usually      remittancâ€¦ consequeâ€¦ stress    better   \n\u001B[38;5;250m 4\u001B[39m software  supporting     aid          example    patents   influencâ€¦ conducted\n\u001B[38;5;250m 5\u001B[39m tourism   private        culture      neverthelâ€¦ dominant  collaborâ€¦ describe \n\u001B[38;5;250m 6\u001B[39m always    export         quantitative sensitive  behind    intelligâ€¦ automatic\n\u001B[38;5;250m 7\u001B[39m interplay substantially  bu",
        "ilds       supports   evaluatiâ€¦ structure validated\n\u001B[38;5;250m 8\u001B[39m partners  awareness      discusses    consensus  italian   accordinâ€¦ describes\n\u001B[38;5;250m 9\u001B[39m studies   customer       sample       kb         suggestiâ€¦ al        gain     \n\u001B[38;5;250m10\u001B[39m evidence  trust          upper        single     consequeâ€¦ drawn     loss     \n\u001B[38;5;246m# â„¹ 3 more variables: `Topic 8` <chr>, `Topic 9` <chr>, `Topic 10` <chr>\u001B[39m\n",
        "> ",
        "m05",
        "\u001B[38;5;246m# A tibble: 4 Ã— 5\u001B[39m\n  `Topic 1`   `Topic 2`   `Topic 3`   `Topic 4` `Topic 5` \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m1\u001B[39m correlation internal    economic    forms     issue     \n\u001B[38;5;250m2\u001B[39m explored    frustration relatedness instead   dependence\n\u001B[38;5;250m3\u001B[39m career      spaces      complexity  p         exit      \n\u001B[38;5;25",
        "0m4\u001B[39m college     type        study       revealed  hand      \n",
        "> ",
        "m04",
        "\u001B[38;5;246m# A tibble: 7 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`   `Topic 3`    `Topic 4`     \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m         \n\u001B[38;5;250m1\u001B[39m economic    often       applications specialization\n\u001B[38;5;250m2\u001B[39m relatedness information building     finding       \n\u001B[38;5;250m3\u001B[39m complexity  relation    proposes     mechanisms    \n\u001B[38;5;250m4\u001B[39m study       application published    anal",
        "yzed      \n\u001B[38;5;250m5\u001B[39m results     ontologies  government   possible      \n\u001B[38;5;250m6\u001B[39m countries   computing   despite      critical      \n\u001B[38;5;250m7\u001B[39m data        estimation  focuses      intervention  \n",
        "> ",
        "m42",
        "\u001B[38;5;246m# A tibble: 4 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`   `Topic 3`   `Topic 4` \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m     \n\u001B[38;5;250m1\u001B[39m interaction economic    individuals engagement\n\u001B[38;5;250m2\u001B[39m major       relatedness labour      determine \n\u001B[38;5;250m3\u001B[39m countrys    complexity  domestic    presence  \n\u001B[38;5;250m4\u001B[39m relatively  study       goals       taking    \n",
        "> ",
        "m10 %>% write_rds(here::here(\"results\", \"m_10x10.rds\"))",
        "> ",
        "m05  %>% write_rds(here::here(\"results\", \"m_05x04.rds\"))",
        "> ",
        "m04  %>% write_rds(here::here(\"results\", \"m_04x07.rds\"))",
        "> ",
        "as.data.frame(m04@beta) %>% as_tibble()",
        "Error in m04@beta : \n  no applicable method for `@` applied to an object of class \"tbl_df\"\n",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 4",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 7)",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 7 Ã— 4\u001B[39m\n  `Topic 1`   `Topic 2`  `Topic 3`  `Topic 4`  \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \n\u001B[38;5;250m1\u001B[39m family      uses       total      economic   \n\u001B[38;5;250m2\u001B[39m innovative  dimensions aim        relatedness\n\u001B[38;5;250m3\u001B[39m robust      climate    especially complexity \n\u001B[38;5;250m4\u001B[39m educational improving  linked     study      \n\u001B[38;5;250m5\u001B[39m policie",
        "s    explores   wordnet    results    \n\u001B[38;5;250m6\u001B[39m certain     higher     young      countries  \n\u001B[38;5;250m7\u001B[39m following   assessment tool       data       \n",
        "> ",
        "set.seed(312)",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 7)",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 7 Ã— 4\u001B[39m\n  `Topic 1`      `Topic 2`    `Topic 3`   `Topic 4`  \n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m       \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m      \n\u001B[38;5;250m1\u001B[39m employees      accuracy     sources     economic   \n\u001B[38;5;250m2\u001B[39m goals          thus         educational relatedness\n\u001B[38;5;250m3\u001B[39m markets        urban        focusing    complexity \n\u001B[38;5;250m4\u001B[39m practices      energy       forms       s",
        "tudy      \n\u001B[38;5;250m5\u001B[39m negatively     environments practice    results    \n\u001B[38;5;250m6\u001B[39m ontology       contrast     emotional   countries  \n\u001B[38;5;250m7\u001B[39m representation lead         plays       data       \n",
        "> ",
        "# Get the topic distribution for each document",
        "> ",
        "doc_topics <- as.data.frame(lda_model_Gibbs@beta) %>% as_tibble()",
        "> ",
        "doc_topics",
        "\u001B[38;5;246m# A tibble: 4 Ã— 11,101\u001B[39m\n      V1    V2     V3     V4     V5     V6     V7    V8     V9   V10    V11\n   \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m  \u001B[3m\u001B[38;5;246m<dbl>\u001B[39m\u001B[23m\n\u001B[38;5;250m1\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[3",
        "1m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m   -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m19\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m \n\u001B[38;5;250m2\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m   -\u001B[31m",
        "9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m22\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m \n\u001B[38;5;250m3\u001B[39m  -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m14\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m  -\u001B[31m8\u001B[39m\u001B[31m.\u001B[39m\u001B[31m49\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m   -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39",
        "m\u001B[31m14\u001B[39m  -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m14\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m  -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m14\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m  -\u001B[31m9\u001B[39m\u001B[31m.\u001B[39m\u001B[31m14\u001B[39m\n\u001B[38;5;250m4\u001B[39m -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m10\u001B[39m\u001B[31m.\u001B[39m\u001B[31m1\u001B[39m -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m10\u001B[39m\u001B[31m.\u001B[39m\u001B[31m5\u001B[39m -\u001B[",
        "31m13\u001B[39m\u001B[31m.\u001B[39m\u001B[31m6\u001B[39m  -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m -\u001B[31m11\u001B[39m\u001B[31m.\u001B[39m\u001B[31m2\u001B[39m \n\u001B[38;5;246m# â„¹ 11,090 more variables: V12 <dbl>, V13 <dbl>, V14 <dbl>, V15 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V16 <dbl>, V17 <dbl>, V18 <dbl>, V19 <dbl>, V20 <dbl>, V21 <dbl>, V22 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V23 <dbl>, V24 <dbl>, V25 <dbl>, V26 <dbl>, V27 <dbl>, V28 <dbl>, V29 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V30 <dbl>, V31 <dbl>, V32 <dbl>, V33 <dbl>, V34 <dbl>, V35 <dbl>, V36 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V37 <dbl>,",
        " V38 <dbl>, V39 <dbl>, V40 <dbl>, V41 <dbl>, V42 <dbl>, V43 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V44 <dbl>, V45 <dbl>, V46 <dbl>, V47 <dbl>, V48 <dbl>, V49 <dbl>, V50 <dbl>,\u001B[39m\n\u001B[38;5;246m#   V51 <dbl>, V52 <dbl>, V53 <dbl>, V54 <dbl>, V55 <dbl>, V56 <dbl>, â€¦\u001B[39m\n\u001B[38;5;246m# â„¹ Use `colnames()` to see all variable names\u001B[39m\n",
        "> ",
        "tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\")",
        "> ",
        "top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "\u001B[38;5;246m# A tibble: 7 Ã— 4\u001B[39m\n  `Topic 1`      `Topic 2`    `Topic 3`\n  \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m          \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m        \u001B[3m\u001B[38;5;246m<chr>\u001B[39m\u001B[23m    \n\u001B[38;5;250m1\u001B[39m employees      accuracy     sources  \n\u001B[38;5;250m2\u001B[39m goals          thus         educatioâ€¦\n\u001B[38;5;250m3\u001B[39m markets        urban        focusing \n\u001B[38;5;250m4\u001B[39m practices      energy       forms    \n\u001B[38;5;250m5\u001B[39m negatively     environments practice \n\u001B[38;5;250m6\u001B[39m ontology       contras",
        "t     emotional\n\u001B[38;5;250m7\u001B[39m representation lead         plays    \n\u001B[38;5;246m# â„¹ 1 more variable: `Topic 4` <chr>\u001B[39m\n",
        "> ",
        "top_w <- top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "> ",
        "pacman::p_load(tidyverse, mallet, tidytext, tm, treemapify, topicmodels)",
        "> ",
        "set.seed(312)",
        "> ",
        "> ",
        "data1 <- read_tsv(here::here(\"data\", \"savedrecs1000.txt\")) %>%  select(TI, AB)",
        "\r\u001B[1mindexing\u001B[0m \u001B[34msavedrecs1000.txt\u001B[0m [] \u001B[32m100.38GB/s\u001B[0m, eta: \u001B[36m 0s\u001B[0m\r                                                                          \r\u001B[1mRows: \u001B[22m\u001B[34m1000\u001B[39m \u001B[1mColumns: \u001B[22m\u001B[34m71\u001B[39m\n\u001B[36mâ”€â”€\u001B[39m \u001B[1mColumn specification\u001B[22m \u001B[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001B[39m\n\u001B[1mDelimiter:\u001B[22m \"\\t\"\n\u001B[31mchr\u001B[39m (31): PT, AU, BE, GP, AF, CA, T...\n\u001B[32mdbl\u001B[39m  (5): PY, VL, SU, BP, PM\n\u001B[33mlgl\u001B[39m (35): BA, BF, BS, LA, DT, DE, I...\n\n\u001B[36mâ„¹\u001B[39m Use `spec",
        "()` to retrieve the full column specification for this data.\n\u001B[36mâ„¹\u001B[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
        "> ",
        "data2 <- read_tsv(here::here(\"data\", \"savedrecs1057.txt\")) %>%  select(TI, AB)",
        "\r\u001B[1mindexing\u001B[0m \u001B[34msavedrecs1057.txt\u001B[0m [] \u001B[32m9.17GB/s\u001B[0m, eta: \u001B[36m 0s\u001B[0m\r                                                                          \r\u001B[1mRows: \u001B[22m\u001B[34m57\u001B[39m \u001B[1mColumns: \u001B[22m\u001B[34m71\u001B[39m\n\u001B[36mâ”€â”€\u001B[39m \u001B[1mColumn specification\u001B[22m \u001B[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001B[39m\n\u001B[1mDelimiter:\u001B[22m \"\\t\"\n\u001B[31mchr\u001B[39m (29): PT, AU, BE, GP, AF, TI, S...\n\u001B[32mdbl\u001B[39m  (5): PY, VL, BP, AR, PM\n\u001B[33mlgl\u001B[39m (37): BA, BF, CA, BS, LA, DT, D...\n\n\u001B[36mâ„¹\u001B[39m Use `spec()` ",
        "to retrieve the full column specification for this data.\n\u001B[36mâ„¹\u001B[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
        "> ",
        "> ",
        "main_data <- bind_rows(",
        "+ ",
        "  data1, data2",
        "+ ",
        ") %>% ",
        "+ ",
        "  filter(!is.na(AB)) %>%   #  REMOVE EMPTY ABSTRACTS FORM THE DATA %>% ",
        "+ ",
        "  distinct(TI, .keep_all = T)",
        "> ",
        "> ",
        "# EXTRACTING ALL ABSTRACTS INTO ONE SINGLE CORPUS",
        "> ",
        "corpus_raw <- main_data %>% ",
        "+ ",
        "  select(AB) %>% ",
        "+ ",
        "  mutate(AB = as.character(AB)) %>% ",
        "+ ",
        "  str_c(collapse = \" \")",
        "Warning message:\nIn stri_c(list(AB = c(\"In recent years economic complexity has grown into an active field of fundamental and applied research. Yet, despite important advances, the policy implications of economic complexity can remain unclear or misunderstood. Here I organize the policy implications of economic complexity in a framework grounded on 4 Ws: what approaches, focused on identifying target activities and/or locations; when approaches, focused on the timing of related and unrelated diversification",
        "; where approaches, focused on the geographic diffusion of knowledge; and who approaches, focused on the role played by agents of structural change. The goal of this paper is to provide a framework that groups, organizes, and clarifies the policy implications of economic complexity and facilitates its continued use in regional and international development.\",  :\n  argument is not an atomic vector; coercing\n",
        "> ",
        "  ",
        "> ",
        "> ",
        "# Step 1: Create a corpus",
        "> ",
        "corpus <- Corpus(VectorSource(corpus_raw))",
        "> ",
        "> ",
        "# Step 2: Text transformation",
        "> ",
        "> ",
        "# convert to lower case",
        "> ",
        "mydata <- tm_map(corpus, content_transformer(tolower))",
        "Warning message:\nIn tm_map.SimpleCorpus(corpus, content_transformer(tolower)) :\n  transformation drops documents\n",
        "> ",
        "# remove anything other than English letters or space",
        "> ",
        "removeNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]\", \"\", x)",
        "> ",
        "mydata <- tm_map(mydata, content_transformer(removeNumPunct))",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, content_transformer(removeNumPunct)) :\n  transformation drops documents\n",
        "> ",
        "# remove stopwords",
        "> ",
        "mydata <- tm_map(mydata, removeWords, stopwords(\"english\"))",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeWords, stopwords(\"english\")) :\n  transformation drops documents\n",
        "> ",
        "#u can create custom stop words using the code below.",
        "> ",
        "myStopwords <- c(setdiff(stopwords('english'), c(\"r\", \"big\")),",
        "+ ",
        "                 \"eeg\", \"les\", \"c\", \"ie\", \"one\", \"e\", \"s\", \"co\", \"would\", \"will\", \"can\", ",
        "+ ",
        "                 \"however\", \"moreover\", \"use\", \"see\", \"used\", \"using\", \"via\", \"amp\", \"ss\", \"algal\",",
        "+ ",
        "                 \"use\", \"two\", \"also\", \"based\", \"key\", \"find\")",
        "> ",
        "mydata <- tm_map(mydata, removeWords, myStopwords)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeWords, myStopwords) :\n  transformation drops documents\n",
        "> ",
        "# remove extra whitespace",
        "> ",
        "mydata <- tm_map(mydata, stripWhitespace)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, stripWhitespace) :\n  transformation drops documents\n",
        "> ",
        "# Remove numbers",
        "> ",
        "mydata <- tm_map(mydata, removeNumbers)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removeNumbers) :\n  transformation drops documents\n",
        "> ",
        "# Remove punctuations",
        "> ",
        "mydata <- tm_map(mydata, removePunctuation)",
        "Warning message:\nIn tm_map.SimpleCorpus(mydata, removePunctuation) :\n  transformation drops documents\n",
        "> ",
        "> ",
        "# inspect(mydata)",
        "> ",
        "> ",
        "# Step 3: Tokenization (optional)",
        "> ",
        "> ",
        "corpus_text <- sapply(mydata, as.character)",
        "> ",
        "tidy_data <- tibble(text = corpus_text)",
        "> ",
        "tidy_data <- tidy_data %>%",
        "+ ",
        "  unnest_tokens(word, text)",
        "> ",
        "> ",
        "# this was not a good idea",
        "> ",
        "# corpus2 <- tm_map(mydata, Boost_tokenizer)",
        "> ",
        "# inspect(corpus2)",
        "> ",
        "# Step 4: Stemming or Lemmatization (optional)",
        "> ",
        "# I FOUND THIS STEP TO CREATE SOME PROBLEMS WITH THE WORDS",
        "> ",
        "# corpus2 <- tm_map(corpus2, stemDocument)  # Stemming",
        "> ",
        "> ",
        "# Step 5: Create Document-Term Matrix (DTM)",
        "> ",
        "dtm <- DocumentTermMatrix(tidy_data)",
        "> ",
        "> ",
        "# inspect(dtm)",
        "> ",
        "> ",
        "# WORD FREQUENCY IN THE CORPUS",
        "> ",
        "words <- tidy_data %>% ",
        "+ ",
        "  group_by(word) %>% ",
        "+ ",
        "  summarise(count= n()) %>% ",
        "+ ",
        "  arrange(desc(count))",
        "> ",
        "  ",
        "> ",
        "> ",
        "# VISUALISATION OF THE FREQUENCY",
        "> ",
        "> ",
        "# MAKE THIS LOOK BETTER",
        "> ",
        "wordsd <- words %>% ",
        "+ ",
        "  top_n(100) %>% ",
        "+ ",
        "  ggplot(aes(area = count, fill = word, label = word)) +",
        "+ ",
        "  guides(fill=\"none\") +",
        "+ ",
        "  geom_treemap() +",
        "+ ",
        "  geom_treemap_text(colour = \"white\",",
        "+ ",
        "                    place = \"centre\",",
        "+ ",
        "                    size = 15) +",
        "+ ",
        "  scale_fill_viridis_d()",
        "\u001B[38;5;232mSelecting by count\u001B[39m\n",
        "> ",
        "> ",
        "write_rds(wordsd, here::here(\"results\", \"wordsd.rds\"))",
        "> ",
        "> ",
        "# SAVE THIS AND USE IT LATER LOOKS GREAT",
        "> ",
        "wordc <- wordcloud2::wordcloud2(words)",
        "> ",
        "> ",
        "write_rds(wordc, here::here(\"results\", \"wordc.rds\"))",
        "> ",
        "> ",
        "> ",
        "## LET'S GO AFTER THE TOPIC MODELLING NOW!",
        "> ",
        "> ",
        "> ",
        "# Convert DTM to matrix",
        "> ",
        "dtm_matrix <- as.matrix(dtm)",
        "> ",
        "> ",
        "> ",
        "> ",
        "###############################################################################",
        "> ",
        "## function to facilitate the configurations",
        "> ",
        "> ",
        "run_LDA <- function(dtm_matrix, k, L) {",
        "+ ",
        "  # Run LDA using Gibbs sampling method",
        "+ ",
        "  lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "+ ",
        "  ",
        "+ ",
        "  # Get the top words for each topic",
        "+ ",
        "  top_words <- terms(lda_model_Gibbs, L)",
        "+ ",
        "  ",
        "+ ",
        "  # Convert top_words to tibble and remove unnecessary characters",
        "+ ",
        "  top_words_tibble <- top_words %>% ",
        "+ ",
        "    as_tibble() %>% ",
        "+ ",
        "    mutate(",
        "+ ",
        "      across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))",
        "+ ",
        "    ) %>% ",
        "+ ",
        "    gt::gt()",
        "+ ",
        "  ",
        "+ ",
        "  # Return the table with topics and their top words",
        "+ ",
        "  return(top_words_tibble)",
        "+ ",
        "}",
        "> ",
        "> ",
        "> ",
        "m10 <- run_LDA(dtm_matrix = dtm_matrix, k = 10, L = 10) ",
        "> ",
        "m05 <- run_LDA(dtm_matrix = dtm_matrix, k = 5, L = 4)   ",
        "> ",
        "# m04 <- run_LDA(dtm_matrix = dtm_matrix, k = 4, L = 7) ",
        "> ",
        "> ",
        "> ",
        "> ",
        "m10 %>% write_rds(here::here(\"results\", \"m_10x10.rds\"))",
        "> ",
        "m05  %>% write_rds(here::here(\"results\", \"m_05x04.rds\"))",
        "> ",
        "# m04  %>% write_rds(here::here(\"results\", \"m_04x07.rds\"))",
        "> ",
        "> ",
        "# Set the number of topics",
        "> ",
        "k <- 4",
        "> ",
        "> ",
        "# Run LDA using MALLET",
        "> ",
        "lda_model_Gibbs <- LDA(dtm_matrix, k, method = \"Gibbs\")",
        "> ",
        "# Get the top words for each topic",
        "> ",
        "top_words_g <- terms(lda_model_Gibbs, 7)",
        "> ",
        "> ",
        "top_w <- top_words_g %>% ",
        "+ ",
        "  as_tibble() %>% ",
        "+ ",
        "  mutate(across(everything(), ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .)))",
        "> ",
        "> ",
        "write_rds(top_w, here::here(\"results\", \"top_w.rds\"))  ",
        "> ",
        "> ",
        "> ",
        "# Get the topic distribution for each document",
        "> ",
        "doc_topics <- as.data.frame(lda_model_Gibbs@beta) %>% as_tibble()",
        "> ",
        "> ",
        "plot  <- tidy(lda_model_Gibbs, matrix = \"beta\") %>% ",
        "+ ",
        "  mutate(across(term, ~ gsub(\"[\\\",\\\\\\\\]\", \"\", .))) %>% ",
        "+ ",
        "  group_by(topic) %>% ",
        "+ ",
        "  slice_max(beta, n = 5) %>% ",
        "+ ",
        "  ungroup() %>%",
        "+ ",
        "  arrange(topic, -beta) %>% ",
        "+ ",
        "  mutate(term = reorder_within(term, beta, topic)) %>%",
        "+ ",
        "  ggplot(aes(beta, term, fill = factor(topic))) +",
        "+ ",
        "  geom_col(show.legend = FALSE) +",
        "+ ",
        "  facet_wrap(~ topic, scales = \"free\") %>% ",
        "+ ",
        "  labs(",
        "+ ",
        "    text = \"Posterior weights of each words in each topic\"",
        "+ ",
        "  )",
        "> ",
        "> ",
        "write_rds(plot, here::here(\"results\", \"weights_plot.rds\"))",
        "\nRestarting R session...\n\n"
    ]
}